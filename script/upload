#!/usr/bin/env ruby
#
# Uploads the contents of `dist/` to Amazon S3.

require 'rubygems'
require 'bundler/setup'
Bundler.require(:default)

require 'pstore'

require_relative '../lib/assets'
require_relative '../lib/logger'
require_relative '../lib/paths'

S3Bucket = 'elections-test'
MaxAge = 365 * 86400 # 1yr
ShortAge = 5

$bucket = Aws::S3::Bucket.new(S3Bucket)

# We cache the sha1s of files we upload, so we don't need to upload twice. This
# shaves a few seconds off our upload times on election night: we won't need to
# re-upload the JavaScript over and over again.
module S3UploadCache
  # finds the cached sha1, or returns nil.
  def self.get_digest(absolute_path)
    @cache.transaction(true) do
      @cache[absolute_path]
    end
  end

  # sets the cached sha1. Do this after upload succeeds.
  def self.set_digest(absolute_path, digest)
    @cache.transaction do
      @cache[absolute_path] = digest
    end
  end

  private

  @cache = PStore.new("#{Paths.Cache}/uploaded-to-s3.pstore")
end

def upload_asset(relative_path, content_type)
  absolute_path = "#{Paths.Dist}/#{relative_path}"
  digest = Assets.digest_file_at_path(absolute_path)
  debug_path = "s3://#{S3Bucket}/#{relative_path}"

  if digest != S3UploadCache.get_digest(absolute_path)
    $logger.info("PUT #{debug_path} #{content_type}")

    File.open(absolute_path, 'r') do |f|
      $bucket.put_object({
        key: relative_path,
        acl: 'public-read',
        body: f,
        cache_control: "public, max-age=#{MaxAge}",
        content_type: content_type,
        expires: Time.now + MaxAge
      })
    end

    S3UploadCache.set_digest(absolute_path, digest)
  else
    $logger.info("SKIP #{debug_path}")
  end
end

def upload_html(absolute_path)
  digest = Assets.digest_file_at_path(absolute_path)
  key = absolute_path[(Paths.Dist.length + 1) .. -6]
  debug_path = "s3://#{S3Bucket}/#{key}"

  if digest != S3UploadCache.get_digest(absolute_path)
    $logger.info("PUT #{debug_path} text/html; charset=utf-8")

    File.open(absolute_path, 'r') do |f|
      $bucket.put_object({
        key: key,
        acl: 'public-read',
        body: f,
        cache_control: "public, max-age=#{ShortAge}",
        content_type: 'text/html; charset=utf-8',
        expires: Time.now + ShortAge
      })
    end

    S3UploadCache.set_digest(absolute_path, digest)
  else
    $logger.info("SKIP #{debug_path}")
  end
end

# Upload the never-expiring assets first. They're fingerprinted, and we never
# delete the old fingerprints. By uploading them first, we guarantee we don't
# upload an HTML file that points to an asset before that asset is uploaded.
Dir["#{Paths.Dist}/**/*.*"].select{ |s| s !~ /\.html$/ }.each do |filename|
  relative_path = filename[(Paths.Dist.length + 1) .. -1]
  content_type = if relative_path =~ /\.css$/
    'text/css; charset=utf-8'
  elsif relative_path =~ /\.png$/
    'image/png'
  elsif relative_path =~ /\.svg$/
    'image/svg+xml'
  elsif relative_path =~ /\.js$/
    'application/javascript; charset=utf-8'
  elsif relative_path =~ /\.json$/
    'application/json'
  else
    raise "Aah, this file shouldn't exist: #{relative_path}"
  end
  upload_asset(relative_path, content_type)
end

# Now upload the HTML files.
Dir["#{Paths.Dist}/**/*.html"].each { |path| upload_html(path) }
